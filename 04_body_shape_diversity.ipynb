{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Body Shape Diversity Metrics\n",
                "## PCA Explained Variance Analysis\n",
                "\n",
                "This notebook measures body shape diversity using pose-based body proportions and PCA analysis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q tensorflow tensorflow-hub numpy scipy matplotlib pillow tqdm scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import tensorflow_hub as hub\n",
                "import numpy as np\n",
                "from scipy import stats\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "import json\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load MoveNet Lightning Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load MoveNet Lightning\n",
                "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
                "model = hub.load(model_url)\n",
                "movenet = model.signatures['serving_default']\n",
                "\n",
                "print(\"MoveNet Lightning loaded!\")\n",
                "\n",
                "# Keypoint names\n",
                "KEYPOINT_NAMES = [\n",
                "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
                "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
                "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
                "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pose Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_preprocess(image_path, input_size=192):\n",
                "    \"\"\"Preprocess image for MoveNet\"\"\"\n",
                "    img = tf.io.read_file(str(image_path))\n",
                "    img = tf.image.decode_image(img, channels=3)\n",
                "    img = tf.image.resize_with_pad(img, input_size, input_size)\n",
                "    img = tf.cast(img, dtype=tf.int32)\n",
                "    return img\n",
                "\n",
                "def extract_keypoints(image_path):\n",
                "    \"\"\"Extract pose keypoints\"\"\"\n",
                "    try:\n",
                "        img = load_and_preprocess(image_path)\n",
                "        img = tf.expand_dims(img, axis=0)\n",
                "        outputs = movenet(img)\n",
                "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
                "        return keypoints\n",
                "    except Exception as e:\n",
                "        return None\n",
                "\n",
                "def extract_all_keypoints(image_dir, max_images=None):\n",
                "    \"\"\"Extract keypoints from all images\"\"\"\n",
                "    image_dir = Path(image_dir)\n",
                "    extensions = ['*.jpg', '*.jpeg', '*.png', '*.webp']\n",
                "    \n",
                "    paths = []\n",
                "    for ext in extensions:\n",
                "        paths.extend(list(image_dir.rglob(ext)))\n",
                "    \n",
                "    if max_images:\n",
                "        paths = paths[:max_images]\n",
                "    \n",
                "    print(f\"Processing {len(paths)} images...\")\n",
                "    \n",
                "    keypoints = []\n",
                "    valid_paths = []\n",
                "    \n",
                "    for path in tqdm(paths):\n",
                "        kp = extract_keypoints(path)\n",
                "        if kp is not None:\n",
                "            keypoints.append(kp)\n",
                "            valid_paths.append(path)\n",
                "    \n",
                "    return np.array(keypoints), valid_paths"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Body Proportion Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_distance(kp, idx1, idx2):\n",
                "    \"\"\"Compute Euclidean distance between two keypoints\"\"\"\n",
                "    p1 = kp[idx1, :2]  # [y, x]\n",
                "    p2 = kp[idx2, :2]\n",
                "    return np.linalg.norm(p1 - p2)\n",
                "\n",
                "def extract_body_proportions(keypoints):\n",
                "    \"\"\"\n",
                "    Extract body proportion features from pose keypoints.\n",
                "    Returns ratios that are scale-invariant.\n",
                "    \"\"\"\n",
                "    # Compute various body segment lengths\n",
                "    \n",
                "    # Torso: shoulder center to hip center\n",
                "    shoulder_center = (keypoints[5, :2] + keypoints[6, :2]) / 2\n",
                "    hip_center = (keypoints[11, :2] + keypoints[12, :2]) / 2\n",
                "    torso_length = np.linalg.norm(shoulder_center - hip_center)\n",
                "    \n",
                "    if torso_length < 0.01:  # Invalid pose\n",
                "        return None\n",
                "    \n",
                "    # Limb lengths\n",
                "    left_upper_arm = compute_distance(keypoints, 5, 7)  # shoulder to elbow\n",
                "    left_forearm = compute_distance(keypoints, 7, 9)    # elbow to wrist\n",
                "    right_upper_arm = compute_distance(keypoints, 6, 8)\n",
                "    right_forearm = compute_distance(keypoints, 8, 10)\n",
                "    \n",
                "    left_thigh = compute_distance(keypoints, 11, 13)    # hip to knee\n",
                "    left_shin = compute_distance(keypoints, 13, 15)     # knee to ankle\n",
                "    right_thigh = compute_distance(keypoints, 12, 14)\n",
                "    right_shin = compute_distance(keypoints, 14, 16)\n",
                "    \n",
                "    # Shoulder and hip width\n",
                "    shoulder_width = compute_distance(keypoints, 5, 6)\n",
                "    hip_width = compute_distance(keypoints, 11, 12)\n",
                "    \n",
                "    # Head-related\n",
                "    head_size = compute_distance(keypoints, 0, 5)  # nose to shoulder (approximation)\n",
                "    \n",
                "    # Compute normalized ratios (relative to torso length)\n",
                "    proportions = [\n",
                "        # Arm ratios\n",
                "        (left_upper_arm + right_upper_arm) / (2 * torso_length),  # avg upper arm ratio\n",
                "        (left_forearm + right_forearm) / (2 * torso_length),      # avg forearm ratio\n",
                "        (left_upper_arm + left_forearm + right_upper_arm + right_forearm) / (4 * torso_length),  # total arm ratio\n",
                "        \n",
                "        # Leg ratios\n",
                "        (left_thigh + right_thigh) / (2 * torso_length),          # avg thigh ratio\n",
                "        (left_shin + right_shin) / (2 * torso_length),            # avg shin ratio\n",
                "        (left_thigh + left_shin + right_thigh + right_shin) / (4 * torso_length),  # total leg ratio\n",
                "        \n",
                "        # Width ratios\n",
                "        shoulder_width / torso_length,                            # shoulder-torso ratio\n",
                "        hip_width / torso_length,                                 # hip-torso ratio\n",
                "        shoulder_width / (hip_width + 1e-6),                      # shoulder-hip ratio\n",
                "        \n",
                "        # Head ratio\n",
                "        head_size / torso_length,                                 # head-torso ratio\n",
                "        \n",
                "        # Symmetry ratios\n",
                "        left_upper_arm / (right_upper_arm + 1e-6),                # arm symmetry\n",
                "        left_thigh / (right_thigh + 1e-6),                        # leg symmetry\n",
                "    ]\n",
                "    \n",
                "    return np.array(proportions)\n",
                "\n",
                "PROPORTION_NAMES = [\n",
                "    'upper_arm_ratio', 'forearm_ratio', 'total_arm_ratio',\n",
                "    'thigh_ratio', 'shin_ratio', 'total_leg_ratio',\n",
                "    'shoulder_torso_ratio', 'hip_torso_ratio', 'shoulder_hip_ratio',\n",
                "    'head_torso_ratio', 'arm_symmetry', 'leg_symmetry'\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Body Shape Diversity Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_pca_variance(proportions):\n",
                "    \"\"\"\n",
                "    Compute PCA explained variance for body proportions.\n",
                "    High variance in multiple components = diverse body shapes.\n",
                "    \"\"\"\n",
                "    # Standardize features\n",
                "    scaler = StandardScaler()\n",
                "    proportions_scaled = scaler.fit_transform(proportions)\n",
                "    \n",
                "    # Apply PCA\n",
                "    pca = PCA()\n",
                "    pca.fit(proportions_scaled)\n",
                "    \n",
                "    return {\n",
                "        'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),\n",
                "        'cumulative_variance_90': int(np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9)) + 1,\n",
                "        'cumulative_variance_95': int(np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95)) + 1,\n",
                "        'singular_values': pca.singular_values_.tolist(),\n",
                "    }, pca, proportions_scaled\n",
                "\n",
                "def compute_proportion_entropy(proportions, num_bins=10):\n",
                "    \"\"\"Compute entropy of body proportion distributions\"\"\"\n",
                "    entropies = []\n",
                "    \n",
                "    for i, name in enumerate(PROPORTION_NAMES):\n",
                "        values = proportions[:, i]\n",
                "        \n",
                "        # Filter outliers\n",
                "        q1, q99 = np.percentile(values, [1, 99])\n",
                "        values = values[(values >= q1) & (values <= q99)]\n",
                "        \n",
                "        if len(values) < 10:\n",
                "            entropies.append(0)\n",
                "            continue\n",
                "        \n",
                "        # Compute histogram\n",
                "        hist, _ = np.histogram(values, bins=num_bins, density=True)\n",
                "        hist = hist[hist > 0]\n",
                "        \n",
                "        # Shannon entropy\n",
                "        entropy = -np.sum(hist * np.log(hist + 1e-10))\n",
                "        entropies.append(entropy)\n",
                "    \n",
                "    return {\n",
                "        'per_feature_entropy': dict(zip(PROPORTION_NAMES, entropies)),\n",
                "        'avg_entropy': float(np.mean(entropies)),\n",
                "    }\n",
                "\n",
                "def compute_proportion_diversity(proportions):\n",
                "    \"\"\"Compute diversity statistics for body proportions\"\"\"\n",
                "    stats = {}\n",
                "    \n",
                "    for i, name in enumerate(PROPORTION_NAMES):\n",
                "        values = proportions[:, i]\n",
                "        stats[name] = {\n",
                "            'mean': float(np.mean(values)),\n",
                "            'std': float(np.std(values)),\n",
                "            'min': float(np.min(values)),\n",
                "            'max': float(np.max(values)),\n",
                "            'range': float(np.max(values) - np.min(values)),\n",
                "        }\n",
                "    \n",
                "    # Overall diversity score (coefficient of variation)\n",
                "    cv_scores = [stats[name]['std'] / (abs(stats[name]['mean']) + 1e-6) \n",
                "                 for name in PROPORTION_NAMES]\n",
                "    \n",
                "    return {\n",
                "        'per_feature_stats': stats,\n",
                "        'avg_coefficient_of_variation': float(np.mean(cv_scores)),\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Load Dataset Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config_path = Path('/content/datasets/dataset_config.json')\n",
                "\n",
                "if config_path.exists():\n",
                "    with open(config_path) as f:\n",
                "        config = json.load(f)\n",
                "    print(\"Loaded dataset configuration\")\n",
                "else:\n",
                "    config = {\n",
                "        'vitonhd': '/content/datasets/vitonhd',\n",
                "        'deepfashion1': '/content/datasets/deepfashion1',\n",
                "        'dresscode': '/content/datasets/dresscode',\n",
                "    }\n",
                "    print(\"Using default paths\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluate Body Shape Diversity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_body_shape_diversity(dataset_name, dataset_path, max_images=500):\n",
                "    \"\"\"Evaluate body shape diversity\"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Evaluating: {dataset_name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    dataset_path = Path(dataset_path)\n",
                "    if not dataset_path.exists():\n",
                "        print(f\"Dataset path not found: {dataset_path}\")\n",
                "        return None, None\n",
                "    \n",
                "    # Extract keypoints\n",
                "    keypoints, paths = extract_all_keypoints(dataset_path, max_images)\n",
                "    \n",
                "    if len(keypoints) == 0:\n",
                "        print(\"No valid poses extracted\")\n",
                "        return None, None\n",
                "    \n",
                "    print(f\"Extracted {len(keypoints)} poses\")\n",
                "    \n",
                "    # Compute body proportions\n",
                "    print(\"\\nExtracting body proportions...\")\n",
                "    proportions = []\n",
                "    for kp in tqdm(keypoints):\n",
                "        prop = extract_body_proportions(kp)\n",
                "        if prop is not None:\n",
                "            proportions.append(prop)\n",
                "    \n",
                "    proportions = np.array(proportions)\n",
                "    print(f\"Valid proportions: {len(proportions)}\")\n",
                "    \n",
                "    if len(proportions) < 10:\n",
                "        print(\"Not enough valid proportions\")\n",
                "        return None, None\n",
                "    \n",
                "    # Compute metrics\n",
                "    print(\"\\nComputing PCA variance...\")\n",
                "    pca_results, pca_model, proportions_scaled = compute_pca_variance(proportions)\n",
                "    \n",
                "    print(\"Computing entropy...\")\n",
                "    entropy_results = compute_proportion_entropy(proportions)\n",
                "    \n",
                "    print(\"Computing diversity stats...\")\n",
                "    diversity_results = compute_proportion_diversity(proportions)\n",
                "    \n",
                "    results = {\n",
                "        'dataset': dataset_name,\n",
                "        'num_samples': len(proportions),\n",
                "        'pca': pca_results,\n",
                "        'entropy': entropy_results,\n",
                "        'diversity': diversity_results,\n",
                "    }\n",
                "    \n",
                "    print(f\"\\nResults for {dataset_name}:\")\n",
                "    print(f\"  - Components for 90% variance: {pca_results['cumulative_variance_90']}\")\n",
                "    print(f\"  - Components for 95% variance: {pca_results['cumulative_variance_95']}\")\n",
                "    print(f\"  - Top-3 variance ratio: {sum(pca_results['explained_variance_ratio'][:3]):.4f}\")\n",
                "    print(f\"  - Avg Entropy: {entropy_results['avg_entropy']:.4f}\")\n",
                "    print(f\"  - Avg CV: {diversity_results['avg_coefficient_of_variation']:.4f}\")\n",
                "    \n",
                "    return results, (proportions, proportions_scaled, pca_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all datasets\n",
                "all_results = {}\n",
                "all_data = {}\n",
                "\n",
                "for name, path in config.items():\n",
                "    if name in ['vitonhd', 'deepfashion1', 'dresscode']:\n",
                "        results, data = evaluate_body_shape_diversity(name.upper(), path, max_images=500)\n",
                "        if results:\n",
                "            all_results[name] = results\n",
                "            all_data[name] = data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA Explained Variance Comparison\n",
                "if all_results:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    colors = {'vitonhd': '#3498db', 'deepfashion1': '#e74c3c', 'dresscode': '#2ecc71'}\n",
                "    \n",
                "    # Individual explained variance\n",
                "    for name, results in all_results.items():\n",
                "        var_ratio = results['pca']['explained_variance_ratio'][:10]\n",
                "        axes[0].plot(range(1, len(var_ratio)+1), var_ratio, \n",
                "                     label=name.upper(), color=colors.get(name, '#333'), marker='o')\n",
                "    \n",
                "    axes[0].set_xlabel('Principal Component')\n",
                "    axes[0].set_ylabel('Explained Variance Ratio')\n",
                "    axes[0].set_title('PCA Explained Variance per Component')\n",
                "    axes[0].legend()\n",
                "    axes[0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Cumulative variance\n",
                "    for name, results in all_results.items():\n",
                "        var_ratio = results['pca']['explained_variance_ratio'][:10]\n",
                "        cumsum = np.cumsum(var_ratio)\n",
                "        axes[1].plot(range(1, len(cumsum)+1), cumsum, \n",
                "                     label=name.upper(), color=colors.get(name, '#333'), marker='o')\n",
                "    \n",
                "    axes[1].axhline(y=0.9, color='gray', linestyle='--', label='90% threshold')\n",
                "    axes[1].axhline(y=0.95, color='gray', linestyle=':', label='95% threshold')\n",
                "    axes[1].set_xlabel('Number of Components')\n",
                "    axes[1].set_ylabel('Cumulative Variance Ratio')\n",
                "    axes[1].set_title('Cumulative PCA Explained Variance')\n",
                "    axes[1].legend()\n",
                "    axes[1].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Body Shape Feature Distributions\n",
                "if all_data:\n",
                "    # Select key features to visualize\n",
                "    key_features = ['shoulder_hip_ratio', 'total_arm_ratio', 'total_leg_ratio', 'shoulder_torso_ratio']\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "    axes = axes.flatten()\n",
                "    \n",
                "    for idx, feature in enumerate(key_features):\n",
                "        feat_idx = PROPORTION_NAMES.index(feature)\n",
                "        \n",
                "        for name, (proportions, _, _) in all_data.items():\n",
                "            values = proportions[:, feat_idx]\n",
                "            axes[idx].hist(values, bins=20, alpha=0.5, label=name.upper(), density=True)\n",
                "        \n",
                "        axes[idx].set_xlabel(feature.replace('_', ' ').title())\n",
                "        axes[idx].set_ylabel('Density')\n",
                "        axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
                "        axes[idx].legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA 2D visualization\n",
                "if all_data:\n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "    \n",
                "    colors = {'vitonhd': '#3498db', 'deepfashion1': '#e74c3c', 'dresscode': '#2ecc71'}\n",
                "    \n",
                "    for name, (_, proportions_scaled, pca_model) in all_data.items():\n",
                "        # Project to 2D\n",
                "        pca_2d = PCA(n_components=2)\n",
                "        coords = pca_2d.fit_transform(proportions_scaled[:200])  # Limit for clarity\n",
                "        \n",
                "        ax.scatter(coords[:, 0], coords[:, 1], \n",
                "                   c=colors.get(name, '#333'), alpha=0.5, label=name.upper(), s=30)\n",
                "    \n",
                "    ax.set_xlabel('PC1')\n",
                "    ax.set_ylabel('PC2')\n",
                "    ax.set_title('Body Shape PCA Projection')\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_path = Path('/content/datasets/body_shape_diversity_results.json')\n",
                "\n",
                "# Convert results to serializable format\n",
                "save_results = {}\n",
                "for name, results in all_results.items():\n",
                "    save_results[name] = {\n",
                "        'dataset': results['dataset'],\n",
                "        'num_samples': results['num_samples'],\n",
                "        'pca_components_90': results['pca']['cumulative_variance_90'],\n",
                "        'pca_components_95': results['pca']['cumulative_variance_95'],\n",
                "        'explained_variance_top3': sum(results['pca']['explained_variance_ratio'][:3]),\n",
                "        'avg_entropy': results['entropy']['avg_entropy'],\n",
                "        'avg_cv': results['diversity']['avg_coefficient_of_variation'],\n",
                "    }\n",
                "\n",
                "with open(results_path, 'w') as f:\n",
                "    json.dump(save_results, f, indent=2)\n",
                "\n",
                "print(f\"Results saved to: {results_path}\")\n",
                "\n",
                "# Summary table\n",
                "print(\"\\n\" + \"=\"*75)\n",
                "print(\"BODY SHAPE DIVERSITY METRICS SUMMARY\")\n",
                "print(\"=\"*75)\n",
                "print(f\"{'Dataset':<15} {'PCA-90%':<10} {'PCA-95%':<10} {'Top3 Var':<12} {'Entropy':<10} {'CV':<10}\")\n",
                "print(\"-\"*67)\n",
                "for name, r in save_results.items():\n",
                "    print(f\"{name:<15} {r['pca_components_90']:<10} {r['pca_components_95']:<10} {r['explained_variance_top3']:<12.4f} {r['avg_entropy']:<10.4f} {r['avg_cv']:<10.4f}\")\n",
                "print(\"=\"*75)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}