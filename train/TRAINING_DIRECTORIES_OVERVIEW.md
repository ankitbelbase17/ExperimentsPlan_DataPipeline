# Training Directories Overview

This document provides a comprehensive overview of all training directories in the SyntheticDatasetExperiments project.

## Directory Structure

```
train/
â”œâ”€â”€ train_CATVTON/              # Concatenation-based Attentive Virtual Try-On
â”œâ”€â”€ train_IDMVTON/              # Image-based Diffusion Model VTON
â”œâ”€â”€ train_CP_VTON/              # Characteristic-Preserving VTON
â”œâ”€â”€ train_VTON_GAN/             # GAN-based Virtual Try-On
â”œâ”€â”€ train_DIT/                  # Diffusion Transformer
â”œâ”€â”€ train_stage_1/              # Stable Diffusion Stage 1
â”œâ”€â”€ train_stage_1_2/            # Stable Diffusion Stages 1-2
â”œâ”€â”€ train_stage_1_2_3/          # Stable Diffusion Stages 1-2-3
â”œâ”€â”€ train_mixture/              # Mixed Dataset Training
â”œâ”€â”€ train_pretrain_DIT/         # DiT Pretraining (Base & Fast variants)
â”œâ”€â”€ train_mask_sapiens_train_mask_agnostic_mask/  # Mask-agnostic training
â””â”€â”€ train_contrastive_diffusion/  # Contrastive learning for diffusion
```

---

## ğŸ¨ Virtual Try-On Methods

### 1. CATVTON (Concatenation-based Attentive VTON)

**Directory:** `train_CATVTON/`

**Architecture:**
- Warping Module: TPS-based garment alignment
- Modified UNet: 16-channel input (person + garment + pose + segmentation)
- Attention-based feature fusion

**Input Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `person_img` | `[B, 3, 512, 512]` | Target person |
| `garment_img` | `[B, 3, 512, 512]` | Garment to try on |
| `pose_map` | `[B, 3, 512, 512]` | Pose keypoints |
| `segmentation` | `[B, 3, 512, 512]` | Body segmentation |

**Output Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `noise_pred` | `[B, 4, 64, 64]` | Predicted noise in latent space |
| `tps_params` | `[B, 50]` | TPS transformation parameters |

**Key Features:**
- Multi-modal concatenation in latent space
- Thin-Plate Spline warping
- Diffusion-based synthesis

---

### 2. IDM-VTON (Improving Diffusion Models for VTON)

**Directory:** `train_IDMVTON/`

**Architecture:**
- CLIP-based Garment Encoder
- SD2-Inpainting UNet (9-channel input)
- Gated Attention Fusion
- DensePose conditioning

**Input Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `person_img` | `[B, 3, 512, 512]` | Target person |
| `garment_img` | `[B, 3, 512, 512]` | Garment image |
| `mask` | `[B, 1, 512, 512]` | Inpainting mask (1=keep, 0=generate) |
| `densepose` | `[B, 3, 512, 512]` | DensePose visualization |

**Output Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `noise_pred` | `[B, 4, 64, 64]` | Predicted noise |
| `garment_features` | `[B, 768]` | CLIP garment embeddings |

**Key Features:**
- Inpainting-based approach
- CLIP garment understanding
- Body-aware generation via DensePose
- Two-stage training (garment encoder â†’ full model)

---

### 3. CP-VTON (Characteristic-Preserving VTON)

**Directory:** `train_CP_VTON/`

**Architecture:**
- Two-stage pipeline:
  1. **GMM (Geometric Matching Module):** Warps garment to person shape
  2. **TOM (Try-On Module):** Synthesizes final result with composition mask

**Stage 1 - GMM:**
| Input | Shape | Output | Shape |
|-------|-------|--------|-------|
| `person_repr` | `[B, 3, 256, 256]` | `tps_params` | `[B, 50]` |
| `garment` | `[B, 3, 256, 256]` | `warped_garment` | `[B, 3, 256, 256]` |

**Stage 2 - TOM:**
| Input | Shape | Output | Shape |
|-------|-------|--------|-------|
| `person` | `[B, 3, 256, 256]` | `tryon_result` | `[B, 3, 256, 256]` |
| `warped_garment` | `[B, 3, 256, 256]` | `composition_mask` | `[B, 1, 256, 256]` |
| `person_repr` | `[B, 3, 256, 256]` | | |

**Loss Functions:**
- L1 reconstruction loss
- VGG perceptual loss
- Composition mask regularization

**Key Features:**
- Explicit geometric matching
- Soft composition masks
- Characteristic preservation

---

### 4. VTON-GAN (GAN-based Virtual Try-On)

**Directory:** `train_VTON_GAN/`

**Architecture:**
- **Generator:** ResNet-based with 6 residual blocks
- **Discriminator:** PatchGAN with spectral normalization

**Generator:**
| Input | Shape | Output | Shape |
|-------|-------|--------|-------|
| `person` | `[B, 3, 256, 256]` | `tryon_result` | `[B, 3, 256, 256]` |
| `garment` | `[B, 3, 256, 256]` | | |
| `pose` | `[B, 3, 256, 256]` | | |

**Discriminator:**
| Input | Shape | Output | Shape |
|-------|-------|--------|-------|
| `image` | `[B, 3, 256, 256]` | `prediction` | `[B, 1, H', W']` |

**Loss Functions:**
- Adversarial loss (LSGAN/vanilla/WGAN-GP)
- L1 reconstruction loss
- Perceptual loss (optional)
- Style loss (optional)

**Key Features:**
- Adversarial training for photorealism
- Spectral normalization for stability
- PatchGAN for high-frequency details

---

## ğŸ¤– Diffusion Transformer (DiT)

### DiT Training

**Directory:** `train_DIT/`

**Architecture:**
- Vision Transformer backbone
- Adaptive Layer Norm (adaLN-Zero)
- Patchification of latent space
- Classifier-free guidance support

**Model Variants:**
| Model | Depth | Hidden Size | Heads | Parameters |
|-------|-------|-------------|-------|------------|
| DiT-S | 12 | 384 | 6 | 33M |
| DiT-B | 12 | 768 | 12 | 130M |
| DiT-L | 24 | 1024 | 16 | 458M |
| DiT-XL | 28 | 1152 | 16 | 675M |

**Input Tensors (Diffusion Objective):**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `images` | `[B, 3, 256, 256]` | RGB images |
| `labels` | `[B]` | Class labels (0-999 for ImageNet) |
| `timesteps` | `[B]` | Diffusion timesteps [0, 1000) |

**Intermediate Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `latents` | `[B, 4, 32, 32]` | VAE-encoded (256/8 = 32) |
| `noisy_latents` | `[B, 4, 32, 32]` | Latents + noise |
| `patches` | `[B, 256, 1152]` | Patchified (32/2)Â² = 256 patches |
| `timestep_emb` | `[B, 1152]` | Sinusoidal timestep embeddings |
| `label_emb` | `[B, 1152]` | Learned label embeddings |
| `conditioning` | `[B, 1152]` | timestep_emb + label_emb |

**Output Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `noise_pred` | `[B, 4, 32, 32]` | Predicted noise |
| `sigma_pred` | `[B, 4, 32, 32]` | Predicted variance (if learn_sigma=True) |

**Training Objectives:**
1. **Diffusion:** `loss = MSE(noise_pred, noise)`
2. **Rectified Flow:** `loss = MSE(v_pred, z1 - z0)`
3. **Flow Matching:** `loss = MSE(v_pred, z1 - z0)`

**Key Features:**
- Transformer-based diffusion
- Classifier-free guidance
- EMA (Exponential Moving Average)
- Large batch training (256+)
- BFloat16 mixed precision
- Gradient clipping

---

## ğŸ“š Stable Diffusion Training

### Stage 1, 1-2, 1-2-3, Mixture

**Directories:** `train_stage_1/`, `train_stage_1_2/`, `train_stage_1_2_3/`, `train_mixture/`

**Architecture:**
- Stable Diffusion v1.5 UNet
- VAE encoder/decoder
- CLIP text encoder

**Input Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `pixel_values` | `[B, 3, 512, 512]` | RGB images |
| `input_ids` | `[B, 77]` | CLIP tokenized text |

**Intermediate Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `latents` | `[B, 4, 64, 64]` | VAE-encoded (scaled by 0.18215) |
| `noise` | `[B, 4, 64, 64]` | Gaussian noise |
| `timesteps` | `[B]` | Random timesteps [0, 1000) |
| `noisy_latents` | `[B, 4, 64, 64]` | Latents + noise |
| `encoder_hidden_states` | `[B, 77, 768]` | CLIP text embeddings |

**Output Tensors:**
| Tensor | Shape | Description |
|--------|-------|-------------|
| `noise_pred` | `[B, 4, 64, 64]` | UNet predicted noise |

**Training Objective:**
```python
loss = MSE(noise_pred, noise)
```

---

## ğŸ¯ Comparison Table: Input/Output Tensors

| Method | Input Modalities | Latent Size | Output | Special Features |
|--------|------------------|-------------|--------|------------------|
| **CATVTON** | Person, Garment, Pose, Seg | `[B, 16, 64, 64]` | Noise | Multi-modal concat, TPS warp |
| **IDM-VTON** | Person, Garment, Mask, DensePose | `[B, 9, 64, 64]` | Noise | Inpainting, CLIP garment |
| **CP-VTON** | Person, Garment, Person Repr | `[B, 9, 256, 256]` | Image + Mask | Two-stage, pixel-space |
| **VTON-GAN** | Person, Garment, Pose | `[B, 9, 256, 256]` | Image | Adversarial, PatchGAN |
| **DiT** | Image, Label | `[B, 256, 1152]` | Noise/Velocity | Transformer, CFG |
| **SD Stages** | Image, Text | `[B, 4, 64, 64]` | Noise | Text-to-image |

---

## ğŸš€ Quick Start Guide

### CATVTON
```bash
cd train_CATVTON
python train.py
```

### IDM-VTON
```bash
cd train_IDMVTON
python train.py
```

### CP-VTON
```bash
cd train_CP_VTON
python train.py
```

### VTON-GAN
```bash
cd train_VTON_GAN
python train.py
```

### DiT
```bash
cd train_DIT
python train.py
```

---

## ğŸ“Š Dataset Requirements

### VTON Methods (CATVTON, IDM-VTON, CP-VTON, VTON-GAN)
```
dataset/
â”œâ”€â”€ person/          # Person images
â”œâ”€â”€ garment/         # Garment images
â”œâ”€â”€ pose/            # Pose keypoints (CATVTON, VTON-GAN)
â”œâ”€â”€ segmentation/    # Body segmentation (CATVTON)
â”œâ”€â”€ mask/            # Inpainting masks (IDM-VTON)
â”œâ”€â”€ densepose/       # DensePose maps (IDM-VTON)
â”œâ”€â”€ person_repr/     # Person representations (CP-VTON)
â”œâ”€â”€ target/          # Ground truth try-on results (CP-VTON, VTON-GAN)
â”œâ”€â”€ train_pairs.txt  # Format: person_id garment_id
â””â”€â”€ val_pairs.txt
```

### DiT
```
imagenet_dataset/
â”œâ”€â”€ train_labels.txt  # Format: image_path class_id
â”œâ”€â”€ val_labels.txt
â””â”€â”€ images/
    â”œâ”€â”€ 0/  # Class folders
    â”œâ”€â”€ 1/
    â””â”€â”€ ...
```

---

## ğŸ”§ Configuration

Each training directory contains a `config.py` file with:
- WandB settings
- Hyperparameters (learning rate, batch size, epochs)
- Model architecture settings
- Dataset paths
- AWS S3 configuration
- Loss weights

---

## ğŸ“ Files in Each Directory

Standard files across all directories:
- `config.py` - Configuration parameters
- `model.py` - Model architecture
- `train.py` - Training script
- `dataloader.py` - Dataset and dataloader
- `utils.py` - Checkpoint and utility functions
- `README.md` - Method-specific documentation

---

## ğŸ“ References

- **CATVTON:** Concatenation-based Attentive Virtual Try-On Network
- **IDM-VTON:** Improving Diffusion Models for Authentic Virtual Try-on in the Wild
- **CP-VTON:** Toward Characteristic-Preserving Image-based Virtual Try-On Network
- **VTON-GAN:** GAN-based approaches for virtual try-on
- **DiT:** Scalable Diffusion Models with Transformers (Peebles & Xie, 2023)
- **Stable Diffusion:** High-Resolution Image Synthesis with Latent Diffusion Models

---

**Last Updated:** 2026-01-22
